# Reproducible experiments {#intro}

We use different technologies to develop experiments. Some examples are [Psychopy](https://www.psychopy.org/), [Qualtrics](https://www.qualtrics.com/), [Limesurvey](https://www.limesurvey.org/), [jsPsych](https://www.jspsych.org/plugins/jspsych-preload/), [Gorilla](https://gorilla.sc/), etc. Each of these has advantages and disadvantages and, in general, there are pragmatic aspects to take into account when adopting one or the other: cost, type of experiment (EEG or behavioral, lab or online), lab history and available resources, ...

In our lab, we opted for [jsPsych](https://www.jspsych.org/) to run behavioral experiments because it is an **open source** javascript library, based on standard web technologies, and can be used online and offline.

In the last years, we started working on a set of tools to help people without coding expertise to create [jsPsych](https://www.jspsych.org/) experimental protocols ([jsPsychMaker](https://github.com/gorkang/jsPsychMaker)), simulate participants ([jspsychMonkeys](https://github.com/gorkang/jsPsychMonkeys)) and standardize and automatize the data preparation and analysis ([jsPsychHelpeR](https://github.com/gorkang/jsPsychHelpeR)).  

Our final goal is to have a big catalog of tasks available to use in the [jsPsychMaker](https://github.com/gorkang/jsPsychMaker) repo. Each of the tasks should run with [jspsychMonkeys](https://github.com/gorkang/jsPsychMonkeys) to create virtual participants. And each task will have a sister script in [jsPsychHelpeR](https://github.com/gorkang/jsPsychHelpeR) to fully automate data preparation (re-coding, reversing items, calculating dimensions, etc.).  


## Open and reproducible pipeline

To replicate an experimental protocol from a publication is not trivial. @obels2020ampps checked the computational reproducibility of Registered Reports in Psychology. From 62 articles meeting the inclusion criteria, only 21 had both data and code, and could be computationally reproduced. One of the main goals of jsPsychR is to be able to create, share and reproduce an experiment, its data, and data preparation and analysis without any extra effort. If recent calls for Journals to assess computational reproducibility are successful [@lindsay2023m], this should be an unavoidable aspect of researcher's work soon enough.  

Furthermore, all the components of the pipeline are be Open Source, which allows reviewers, collaborators, etc. to check and run the code. This also makes it accessible to anyone with a computer connected to the internet, eliminating cost constrains.  

With this system you can create a paradigm, simulate data and prepare data and analysis almost automatically.

The system output is standardized, so names of variables and the structure of the data are predictable. Finally, the plots, tables, reports and analysis are reproducible, so you can get everything ready with simulated data, preregister or even better, go for a [registered report](https://www.cos.io/initiatives/registered-reports) and just relaunch the data preparation and analysis when the participant's responses arrive, with a single command.

And if you want to share the final data preparation and analysis project in a Docker container to make sure the future generations will be able to run it without dependency issues, [we got you covered](05-jsPsychHelpeR.html#docker-containers).  


## Automatization

We tried to get a few basic things right, but this is an evolving project, and some things are more complex than one would want. Please do report the issues you find:

-   [jsPsychMaker issues](https://github.com/gorkang/jsPsychMaker)
-   [jsPsychMonkeys issues](https://github.com/gorkang/jsPsychMonkeys/issues)
-   [jsPsychHelpeR issues](https://github.com/gorkang/jsPsychHelpeR/issues)


```{r, echo=FALSE, out.width='40%', fig.cap='SOURCE: https://xkcd.com/1425/', fig.align="center"}
  knitr::include_graphics(here::here("img/xkcd_tasks.png"))
```
