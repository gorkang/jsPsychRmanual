# jsPsychHelpeR {#jsPsychHelpeR}

------------------------------------------------------------------------

[jsPsychHelpeR](https://github.com/gorkang/jsPsychHelpeR): Standardize and automatize data preparation and analysis of jsPsych experiments created with jsPsychMaker.

------------------------------------------------------------------------

jsPsychHelpeR will lend you a hand automatizing and standardizing your data preparation and analysis.

-   Use a completely open, reproducible and automatic process to prepare your data

-   Data preparation ready for \> 50 tasks (see [here the list of tasks](https://github.com/gorkang/jsPsychMaker/blob/main/canonical_protocol/canonical_protocol_details.csv))

-   Get tidy output dataframes for each task, and tidy general dataframes for the whole protocol

-   Include tests for common issues

-   Automatic reports with progress, descriptives, codebook, etc.

------------------------------------------------------------------------

**See [QuickGuide](#QuickGuidejsPsychHelpeR) for basic instructions.**

------------------------------------------------------------------------

## How to prepare data

Our goal is that [jsPsychMaker](https://github.com/gorkang/jsPsychMaker) task has a sister script on [jsPsychHelpeR](https://github.com/gorkang/jsPsychHelpeR) to help prepare the data automatically. If a task you need does not have one, you can try to [create the script yourself](https://gorkang.github.io/jsPsychR-manual/jsPsychHelpeR.html#create-new-tasks) and do a [pull request in the jsPsychHelpeR repo](https://github.com/gorkang/jsPsychHelpeR/pulls), or fill the [NEW tasks document](https://docs.google.com/spreadsheets/d/1LAsyTZ2ZRP_xLiUBkqmawwnKWgy8OCwq4mmWrrc_rpQ/edit#gid=0) with the details to help us create the correction script.

If you already ran a pilot experiment, simply:

1.  Download `jsPsychHelpeR`:

```{r eval=FALSE}
if (!require('usethis')) install.packages('usethis'); library('usethis')
usethis::use_course("gorkang/jsPsychHelpeR")
```

2.  `run_initial_setup()`: go to the file `run.R` and run the following two lines. They will:

-   Try to make sure you have all the dependencies, folders, etc.\
-   Download all the data from your protocol (you will need the FTP credentials and set `download_files = TRUE`)\
-   Download and zip a copy of the full protocol without the data (you will need the FTP credentials and set `download_task_script = TRUE`)\
-   Create a customized `_targets.R` file adapted to the data of your protocol, so data preparation can run automagically

```{r eval=FALSE}
invisible(lapply(list.files("./R", full.names = TRUE, pattern = ".R$"), source))
run_initial_setup(pid = "999", download_files = TRUE, download_task_script = TRUE)
```

This should work on [Ubuntu](https://ubuntu.com/), if you have the [FTP credentials](jsPsychHelpeR.html#run_initial_setup), and `sshpass` and `rsync` installed.

If you don't have the credentials, or some of the requirements to download the files, you can manually copy the .csv files to the `data/PROJECT ID` folder, and then run the `offline` version of `run_initial_setup()`:

```{r eval=FALSE}
invisible(lapply(list.files("./R", full.names = TRUE, pattern = ".R$"), source))
run_initial_setup(pid = "999", download_files = FALSE, download_task_script = FALSE)
```

------------------------------------------------------------------------

### Targets pipeline

To help make the pipeline reproducible and more efficient, we use the [targets](https://github.com/wlandau/targets) package [@targets]. A few basic things to know:

-   **The whole process can be reproduced running `targets::tar_make()`**

-   A nice visualization of all the pre-processing steps can be seen with `targets::tar_visnetwork(targets_only = TRUE)`

-   The file `_targets.R` contains the important parameters and calls to all the functions used when running `targets::tar_make()`

To see more detail about any specific step, you can:

1.  Go to the relevant function in `_targets.R` (cursor on a function, then F2)

2.  Load the input parameters of the function with `debug_function(NAME_OF FUNCTION)`. Alternatively, manually use `targets::tar_load(NAME_OF_TARGET)`

3.  Run the code step by step as you would normally do

## Basics

[jsPsychHelpeR](https://github.com/gorkang/jsPsychHelpeR) uses as input a data created with a [jsPsychMaker](https://github.com/gorkang/jsPsychMaker) experimental protocol.

### Inputs

The input data folder will be named after the protocol_id, for example `999/` and needs to be placed in the `data/` folder of the jsPsychHelpeR project `data/YOUR_PROJECT_NUMBER`:

-   The data folder can contain either multiple .csv files, or a single .zip file

There will be a single .csv file for each participant and task of the protocol. For example:

-   999_Consent_original_2022-04-02T205622_1.csv:
    -   \[project: 999\]\_\[experimento: Consent\]\_\[version: original\]\_\[datetime: 2022-04-02T205622\]\_\[participant id: 1\]

### Outputs

When the pipeline successfully runs with `targets::tar_make()`, a number of outputs will be created.

All the outputs can be found in the `/outputs` folder. The only exception is the sensitive data and reports, which can be found in `.vault/outputs`. `r kableExtra::text_spec("WARNING: The '.vault/' folder **MUST NOT** be made public.", color = "red")`

#### Output folders

The outputs will be organized in different folders:

-   **Data frames** for different stages of data processing can be found in `outputs/data`

-   **Temporary files for manual correction** are in `outputs/data/manual_correction` (the final manual correction files must be place by the user in `data/manual_correction`). `r kableExtra::text_spec("WARNING: These will be overwritten each time the pipeline runs", color = "red")`

-   **Plots**, **tables** and **reports** are in `outputs/plots`, `outputs/tables`and `outputs/reports` respectively.

-   **Test outputs** are in `outputs/tests_outputs`

-   **Anonymized Raw data** will be moved to `.vault/data_vault/`

#### Output dataframes

There will be a single data frame (df) for each of the tasks in `outputs/data`, plus a data frame (DF) for each of the steps of the data preparation, and a dictionary file listing all the available tasks. We store the files in two formats, csv and rds:

-   **DF_raw.csv**: All the `data/project_id/` csv files combined on a single file. We only add the columns "project", "experimento", "version", "datetime", "id" by parsing the filenames

-   **DF_clean.csv**: Clean version of the raw file ready to process the individual tasks

-   **df_ShortNameOfTask.csv**: One df for each of the tasks of the protocol after being processed with the `prepare_ShortNameOfTask()` functions

-   **DF_joined.csv**: all the processed tasks joined in a single DF

-   **DF_analysis**: only the total scores and dimensions from `DF_joined` (columns ending in `_DIRt`, `_STDt`, `_DIRd`, `_RELd`, `STDd`). Can be visually explored using the shiny app in `Rmd/app.R`

-   **DICCIONARY_tasks.csv**: list of all tasks in the protocol

#### Output dataframes column names

All the output processed data frames columns are named in a standardized way:

-   **ShortNameOfTask_ItemNumber_RAW**: raw responses of participants for individual items

-   **ShortNameOfTask_ItemNumber_DIR**: processed raw responses following the task correction instructions (e.g. inverting certain items, converting strings to numbers, computing accuracy...)

-   **ShortNameOfTask_RAW_NA**: number of missing data (NA) in the RAW responses

-   **ShortNameOfTask_DIR_NA**: number of missing data (NA) in the DIR responses. If it is not equal to `ShortNameOfTask_RAW_NA` there is something wrong in the items correction.

-   **ShortNameOfTask_DimensionName_DIRd**: scores for a specific dimension (`d`) in a task, calculated following task correction instructions (e.g. summing or averaging certain items)

-   **ShortNameOfTask_DimensionName_RELd**: scores for a specific dimension (`d`) in a task, calculated following task correction instructions AND after filtering items with low reliability. See [Reliability section](jsPsychHelpeR.html#reliability) for more information.

-   **ShortNameOfTask_DimensionName_STDd**: standardized score for a dimension (`d`)

-   **ShortNameOfTask_DIRt**: total (`t`) score for a task calculated following task correction instructions (e.g. summing or averaging all items)

-   **ShortNameOfTask_STDt**: standardized (`t`) score for a task

## Advanced

### Create your own reports

You can use any of the template reports in the `_targets.R` file, or create your own reports.

We will start opening one of the template reports: `rstudioapi::navigateToFile("doc/report_analysis.Rmd")`.

-   Edit the RMarkdown file to adapt it to your needs.

-   If you already did `targets::tar_make()`, when running `targets::tar_load(DF_analysis)` the dataframe `DF_analysis` will load in your Environment.

Go back to the `_targets.R` file:

-   Look for `# Analysis report` and uncomment the following lines:

```{r eval=FALSE}
# tar_render(report_analysis, "doc/report_analysis.Rmd",
#            output_file = paste0("../outputs/reports/report_analysis.html")),
```

When you finished editing and uncomented the tar_render command, go back to the `run.R` file:

-   `targets::tar_make()`

### Create new tasks

To create the correction script for a new task, you start with:

-   `create_new_task(short_name_task = "NAMETASK")`

This will:

-   create a new file from a template correction script (`R_tasks/prepare_TEMPLATE.R`)

-   adapt it to your `short_name_task` to make everything as standardized as possible

-   open the new `prepare_NAMETASK.R` file

If the parameter `get_info_googledoc = TRUE`:

-   The [NEW tasks document](https://docs.google.com/spreadsheets/d/1LAsyTZ2ZRP_xLiUBkqmawwnKWgy8OCwq4mmWrrc_rpQ/edit#gid=0) is checked.

-   If the document has been filled properly, it will show in the console standardized strings (ready to be copy/pasted to the new `prepare_NAMETASK.R` file) about:

    -   dimension names

    -   items corresponding to each dimension

    -   dimension calculation

    -   inverse items

    -   numeric conversion of items

You can also use `get_dimensions_googledoc()` as a standalone function:

`get_dimensions_googledoc(short_name_text = "MLQ")`

All the `prepare_NAMEOFTASK.R` scripts on the `R_tasks/` folder have been created starting from the same template. The only exception are the experimental tasks and some surveys with particularities that require more complex adaptations.

When you finish implementing the correction script, please do a [Pull request](https://github.com/gorkang/jsPsychHelpeR/pulls) so we can add you script to the pool. If you have not already, please help us filling up details about the task in the [NEW tasks document](https://docs.google.com/spreadsheets/d/1LAsyTZ2ZRP_xLiUBkqmawwnKWgy8OCwq4mmWrrc_rpQ/edit#gid=0).

### Adapting new tasks

`get_dimensions_googledoc` will show you how to adapt the `prepare_TASK()` script, but you will need to know how it works to be able to edit the relevant bits. Also, sometimes `get_dimensions_googledoc` won't get all the details of the task right, or there could be non-standard elements to it. Here, we will describe some of the elements of the template to help understand how it works.

Remember you should **ALWAYS** start with `create_new_task(short_name_task = "NAMETASK")` so your task template works well with `jsPsychHelpeR`.

There are three chunks you will need to adapt to have a fully working preparation script.

-   `[ADAPT 1/3]: Items to ignore and reverse, dimensions`
-   `[ADAPT 2/3]: RAW to DIR for individual items`
-   `[ADAPT 3/3]: Scales and dimensions calculations`

#### Items to ignore and reverse, dimensions

```{r eval=FALSE}
# [ADAPT 1/3]: Items to ignore and reverse, dimensions -----------------------
# ****************************************************************************

description_task = "" # Brief description here

items_to_ignore = c("000") # Ignore these items: If nothing to ignore, keep as is
items_to_reverse = c("000") # Reverse these items: If nothing to reverse, keep as is

## NameDimension1, NameDimension2 should be the names of the dimensions
## Inside each c() create a vector of the item numbers for the dimension
## Add lines as needed. If there are no dimensions, keep as is
items_dimensions = list(
  NameDimension1 = c("000"),
  NameDimension2 = c("000")
)

# [END ADAPT 1/3]: ***********************************************************
# ****************************************************************************
```

#### RAW to DIR for individual items

```{r eval=FALSE}
DF_long_DIR = 
    DF_long_RAW %>% 
    select(id, trialid, RAW) %>%
    
    
    
  # [ADAPT 2/3]: RAW to DIR for individual items -------------------------------
  # ****************************************************************************
  
    # Transformations
    mutate(
      DIR =
        case_when(
          RAW == "Nunca" ~ 1,
          RAW == "Poco" ~ 2,
          RAW == "Medianamente" ~ 3,
          RAW == "Bastante" ~ 4,
          RAW == "Mucho" ~ 5,
          is.na(RAW) ~ NA_real_,
          grepl(items_to_ignore, trialid) ~ NA_real_,
          TRUE ~ 9999
        )
    ) %>% 
    
    # Invert items
    mutate(
      DIR = 
        case_when(
          DIR == 9999 ~ DIR, # To keep the missing values unchanged
          trialid %in% paste0(short_name_scale_str, "_", items_to_reverse) ~ (6 - DIR),
          TRUE ~ DIR
        )
    )
    
  # [END ADAPT 2/3]: ***********************************************************
  # ****************************************************************************
    

```

#### Scales and dimensions calculations

```{r eval=FALSE}
# [ADAPT 3/3]: Scales and dimensions calculations ----------------------------
# ****************************************************************************

# Reliability -------------------------------------------------------------
# REL1 = auto_reliability(DF_wide_RAW, short_name_scale = short_name_scale_str, items = items_DIRd1)
# items_RELd1 = REL1$item_selection_string
  

# [USE STANDARD NAMES FOR Scales and dimensions: names_list$name_DIRd[1], names_list$name_DIRt,...] 
# CHECK with: create_formulas(type = "dimensions_DIR", functions = "sum", names_dimensions)
DF_wide_RAW_DIR =
  DF_wide_RAW %>% 
  mutate(

    # [CHECK] Using correct formula? rowMeans() / rowSums()
    
    # Score Dimensions (see standardized_names(help_names = TRUE) for instructions)
    !!names_list$name_DIRd[1] := rowMeans(select(., paste0(short_name_scale_str, "_", items_dimensions[[1]], "_DIR")), na.rm = TRUE), 
    !!names_list$name_DIRd[2] := rowSums(select(., paste0(short_name_scale_str, "_", items_dimensions[[2]], "_DIR")), na.rm = TRUE),
    
    # Reliability Dimensions (see standardized_names(help_names = TRUE) for instructions)
    # !!names_list$name_RELd[1] := rowMeans(select(., paste0(short_name_scale_str, "_", items_RELd1, "_DIR")), na.rm = TRUE), 

    # Score Scale
    !!names_list$name_DIRt := rowSums(select(., matches("_DIR$")), na.rm = TRUE)
    
  )
  
# [END ADAPT 3/3]: ***********************************************************
# ****************************************************************************
```

### How to fill the [NEW tasks document](https://docs.google.com/spreadsheets/d/1LAsyTZ2ZRP_xLiUBkqmawwnKWgy8OCwq4mmWrrc_rpQ/edit#gid=0)

The best way is to check the main document with information about all the tasks ([Tareas jsPsychR](https://docs.google.com/spreadsheets/d/1Eo0F4GcmqWZ1cghTpQlA4aHsc8kTABss-HAeimE2IqA/edit#gid=0)) and find a similar task to copy/paste and adapt it in the [NEW tasks document](https://docs.google.com/spreadsheets/d/1LAsyTZ2ZRP_xLiUBkqmawwnKWgy8OCwq4mmWrrc_rpQ/edit#gid=0).

The main suggestion is to be very consistent. For example, when entering the informacion about numeric conversion in the Puntajes_items tab:

    All the cells must be:  
    1 = Mucho  
    2 = Poco  
    ...

DO NOT do things like:

    1: Mucho  
    1 Mucho  
    1 pto = Mucho  
    Mucho 1  

Please, make sure you fill out all the details in all the tabs.

### DEBUG tasks

At the begining of each of the `R_tasks/prepare_NAMETASK.R` scripts you will find a commented `debug_function(prepare_NAMETASK)` line.

When running it, it will load the input parameters for the task. From there, you can work inside of the preparation scipt as you would normally do in a R script.

If you get the error `"Error in debug_function(prepare_NAMETASK) : could not find function 'debug_function'`debug_function()`does nor work"` you will need to load all the functions in the `R/` folder first.

You can do this in one of three ways:

-   `CONTROL + P` shortcut will work if the `run_initial_setup()` completed correctly (at least on Ubuntu systems).

-   Run `targets::tar_load_globals()`

-   Or directly, source all the scripts in the `R/` folder: `invisible(lapply(list.files("./R", full.names = TRUE, pattern = ".R$"), source))`

## Helper functions

### Reliability

You can use the `auto_reliability()` function to help you automatically filter items with low reliability (although doing this automatically is probably a bad idea). The function uses `psych::alpha()` and filters by default items with an r.drop \<= 0.2. See `psych::alpha()` help for more details. **IMPORTANT**: Using `psych::omega()` is generally a better idea, see [the alpha help page](https://search.r-project.org/CRAN/refmans/psych/html/alpha.html).

An example can be found in [prepare_REI40()](https://github.com/gorkang/jsPsychHelpeR/blob/master/R_tasks/prepare_REI40.R).

The basic logic would be:

```{r eval=FALSE}
# Define items for a specific dimension
items_DIRd1 = c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10")

# Calculate reliability
REL1 = auto_reliability(DF_wide_RAW, short_name_scale = short_name_scale_str, items = items_DIRd1, min_rdrop = 0.2)

# Store item selection in a variable
items_RELd1 = REL1$item_selection_string

# In the final Dimension calculation, use the item selection including only the items with a reliability over the defined threshold   
## See `items_RELd1` below
!!names_list$name_RELd[1] := rowMeans(select(., paste0(short_name_scale_str, "_", items_RELd1, "_DIR")), na.rm = TRUE), 

# Compare it with the calculation including the original items
## See `items_DIRd1` below
!!names_list$name_DIRd[1] := rowMeans(select(., paste0(short_name_scale_str, "_", items_DIRd1, "_DIR")), na.rm = TRUE), 

```

## Technical aspects

### How trialid's are processed

See `PRFBM`:

-   If more than one response per screen
    -   Item: `PRFBM_04`
    -   Responses: `{"daño":"Parcialmente en desacuerdo","beneficio":"Parcialmente en desacuerdo"}`
    -   final trialids: `PRFBM_04_beneficio` and `PRFBM_04_daño`

## Common ERRORS

### `run_initial_setup()`:

    x Can find server credentials in '.vault/.credentials'
    x 0 tasks found for protocol 'TU NUMERO DE PROYECTO'. NOT creating _targets.R file

#### On Linux (Ubuntu):

-   IF you have the server credentials:

    -   Open .credentials_TEMPLATE `rstudioapi::navigateToFile(".vault/.credentials_TEMPLATE")`

    -   Edit the file with your server credentials

    -   Rename the file to `.credentials`

------------------------------------------------------------------------

-   IF you DON'T have the credentials but you have the .csv results files:

    -   Copy the csv files to the folder `data/YOUR_PROJECT_NUMBER`

    -   Run again `run_initial_setup()`

### On Mac or Windows:

-   Copy the csv files to the folder `data/YOUR_PROJECT_NUMBER`

-   Run again `run_initial_setup()`
